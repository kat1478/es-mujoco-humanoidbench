{
  "experiment": "h1hand-push-v0",
  "config": {
    "env_name": "h1hand-push-v0",
    "population_size": 40,
    "sigma": 0.02,
    "learning_rate": 0.01,
    "max_episode_steps": 1000,
    "seed": 42
  },
  "results": {
    "final_reward_mean": -258.3291589044001,
    "final_reward_std": 72.057809541981,
    "best_reward": -201.1376495361328,
    "total_timesteps": 1018230,
    "total_iterations": 52,
    "total_time_seconds": 4027.7618436813354
  },
  "history": [
    {
      "iteration": 1,
      "timesteps": 20000,
      "mean_reward": -314.59552001953125,
      "max_reward": -131.52064514160156,
      "min_reward": -2075.654296875,
      "std_reward": 294.2262268066406,
      "best_reward": -314.59552001953125,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 2,
      "timesteps": 40000,
      "mean_reward": -313.7021789550781,
      "max_reward": -77.639404296875,
      "min_reward": -2042.9925537109375,
      "std_reward": 310.3126220703125,
      "best_reward": -313.7021789550781,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 3,
      "timesteps": 59002,
      "mean_reward": -201.1376495361328,
      "max_reward": 999.9182739257812,
      "min_reward": -769.31103515625,
      "std_reward": 293.8938903808594,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19002
    },
    {
      "iteration": 4,
      "timesteps": 79002,
      "mean_reward": -341.42388916015625,
      "max_reward": -152.10060119628906,
      "min_reward": -1833.173828125,
      "std_reward": 292.4913635253906,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 5,
      "timesteps": 99002,
      "mean_reward": -316.4726257324219,
      "max_reward": -133.2244110107422,
      "min_reward": -2069.7568359375,
      "std_reward": 348.0449523925781,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 6,
      "timesteps": 119002,
      "mean_reward": -294.03558349609375,
      "max_reward": -142.5489959716797,
      "min_reward": -1080.4906005859375,
      "std_reward": 139.60096740722656,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 7,
      "timesteps": 139002,
      "mean_reward": -545.1506958007812,
      "max_reward": -92.19780731201172,
      "min_reward": -2604.6015625,
      "std_reward": 646.1466674804688,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 8,
      "timesteps": 158508,
      "mean_reward": -599.0386962890625,
      "max_reward": 999.0985107421875,
      "min_reward": -2401.181640625,
      "std_reward": 633.9739379882812,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19506
    },
    {
      "iteration": 9,
      "timesteps": 178508,
      "mean_reward": -258.7201232910156,
      "max_reward": -98.33950805664062,
      "min_reward": -354.556640625,
      "std_reward": 65.91636657714844,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 10,
      "timesteps": 198009,
      "mean_reward": -234.6824951171875,
      "max_reward": 999.9244384765625,
      "min_reward": -583.2564697265625,
      "std_reward": 214.23609924316406,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 11,
      "timesteps": 217510,
      "mean_reward": -297.11224365234375,
      "max_reward": 999.9510498046875,
      "min_reward": -2077.581298828125,
      "std_reward": 404.5661926269531,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 12,
      "timesteps": 236524,
      "mean_reward": -288.1090393066406,
      "max_reward": 999.9324951171875,
      "min_reward": -1443.6878662109375,
      "std_reward": 389.9957275390625,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19014
    },
    {
      "iteration": 13,
      "timesteps": 255046,
      "mean_reward": -505.9969787597656,
      "max_reward": 999.4701538085938,
      "min_reward": -3193.860595703125,
      "std_reward": 764.0670776367188,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 18522
    },
    {
      "iteration": 14,
      "timesteps": 275046,
      "mean_reward": -525.21875,
      "max_reward": -163.3238067626953,
      "min_reward": -3348.763427734375,
      "std_reward": 630.7750244140625,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 15,
      "timesteps": 293556,
      "mean_reward": -226.6929168701172,
      "max_reward": 999.9396362304688,
      "min_reward": -1543.6358642578125,
      "std_reward": 433.1396484375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 18510
    },
    {
      "iteration": 16,
      "timesteps": 312558,
      "mean_reward": -313.6201171875,
      "max_reward": 999.915283203125,
      "min_reward": -1899.3013916015625,
      "std_reward": 473.1343994140625,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19002
    },
    {
      "iteration": 17,
      "timesteps": 332059,
      "mean_reward": -267.76025390625,
      "max_reward": 999.9224853515625,
      "min_reward": -1604.1295166015625,
      "std_reward": 330.2604675292969,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 18,
      "timesteps": 352059,
      "mean_reward": -444.88519287109375,
      "max_reward": -146.69691467285156,
      "min_reward": -4471.095703125,
      "std_reward": 822.4019165039062,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 19,
      "timesteps": 372059,
      "mean_reward": -462.69000244140625,
      "max_reward": -118.786376953125,
      "min_reward": -2355.866943359375,
      "std_reward": 482.47412109375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 20,
      "timesteps": 391065,
      "mean_reward": -333.4599609375,
      "max_reward": 999.9178466796875,
      "min_reward": -2927.1923828125,
      "std_reward": 562.1629638671875,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19006
    },
    {
      "iteration": 21,
      "timesteps": 410088,
      "mean_reward": -326.9169921875,
      "max_reward": 998.21044921875,
      "min_reward": -2346.066650390625,
      "std_reward": 515.0324096679688,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19023
    },
    {
      "iteration": 22,
      "timesteps": 430088,
      "mean_reward": -307.28680419921875,
      "max_reward": -113.65491485595703,
      "min_reward": -1038.1148681640625,
      "std_reward": 159.87086486816406,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 23,
      "timesteps": 449108,
      "mean_reward": -449.2916564941406,
      "max_reward": 999.3864135742188,
      "min_reward": -1459.518310546875,
      "std_reward": 495.2395935058594,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19020
    },
    {
      "iteration": 24,
      "timesteps": 468652,
      "mean_reward": -225.7328643798828,
      "max_reward": 988.8950805664062,
      "min_reward": -372.37347412109375,
      "std_reward": 203.977783203125,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19544
    },
    {
      "iteration": 25,
      "timesteps": 488097,
      "mean_reward": -215.7038116455078,
      "max_reward": 999.9193725585938,
      "min_reward": -837.505126953125,
      "std_reward": 284.6977844238281,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19445
    },
    {
      "iteration": 26,
      "timesteps": 508097,
      "mean_reward": -232.6124725341797,
      "max_reward": -128.982421875,
      "min_reward": -339.7770080566406,
      "std_reward": 60.1335563659668,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 27,
      "timesteps": 528097,
      "mean_reward": -234.4599609375,
      "max_reward": -101.55860137939453,
      "min_reward": -399.8464660644531,
      "std_reward": 69.54200744628906,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 28,
      "timesteps": 547598,
      "mean_reward": -292.3074645996094,
      "max_reward": 999.9281616210938,
      "min_reward": -1515.67041015625,
      "std_reward": 338.89141845703125,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 29,
      "timesteps": 567099,
      "mean_reward": -254.6380615234375,
      "max_reward": 999.9379272460938,
      "min_reward": -1514.2408447265625,
      "std_reward": 289.15374755859375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 30,
      "timesteps": 586120,
      "mean_reward": -674.575439453125,
      "max_reward": 999.92578125,
      "min_reward": -6180.869140625,
      "std_reward": 1444.4114990234375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19021
    },
    {
      "iteration": 31,
      "timesteps": 605621,
      "mean_reward": -295.3331604003906,
      "max_reward": 999.9166870117188,
      "min_reward": -1977.4410400390625,
      "std_reward": 375.9806213378906,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 32,
      "timesteps": 625621,
      "mean_reward": -292.2816467285156,
      "max_reward": -128.62330627441406,
      "min_reward": -1321.654296875,
      "std_reward": 176.27865600585938,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 33,
      "timesteps": 645621,
      "mean_reward": -346.2539978027344,
      "max_reward": -130.71128845214844,
      "min_reward": -3067.837158203125,
      "std_reward": 446.63555908203125,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 34,
      "timesteps": 665621,
      "mean_reward": -289.3223571777344,
      "max_reward": -123.43650817871094,
      "min_reward": -1603.962158203125,
      "std_reward": 221.6531982421875,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 35,
      "timesteps": 685122,
      "mean_reward": -463.89312744140625,
      "max_reward": 999.91943359375,
      "min_reward": -4040.915283203125,
      "std_reward": 831.6538696289062,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 36,
      "timesteps": 705122,
      "mean_reward": -257.35675048828125,
      "max_reward": -147.8408966064453,
      "min_reward": -352.85675048828125,
      "std_reward": 55.94158172607422,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 37,
      "timesteps": 724140,
      "mean_reward": -279.48846435546875,
      "max_reward": 999.9240112304688,
      "min_reward": -1804.8367919921875,
      "std_reward": 430.26531982421875,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19018
    },
    {
      "iteration": 38,
      "timesteps": 743641,
      "mean_reward": -262.31201171875,
      "max_reward": 999.909423828125,
      "min_reward": -1442.5751953125,
      "std_reward": 326.24609375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 39,
      "timesteps": 763641,
      "mean_reward": -375.0734558105469,
      "max_reward": -130.81744384765625,
      "min_reward": -2196.821533203125,
      "std_reward": 405.62286376953125,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 40,
      "timesteps": 783641,
      "mean_reward": -337.8647155761719,
      "max_reward": -151.65643310546875,
      "min_reward": -1546.02685546875,
      "std_reward": 266.10546875,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 41,
      "timesteps": 803641,
      "mean_reward": -327.7345275878906,
      "max_reward": -135.0200653076172,
      "min_reward": -1621.1929931640625,
      "std_reward": 274.6880798339844,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 42,
      "timesteps": 823641,
      "mean_reward": -321.54681396484375,
      "max_reward": -133.53985595703125,
      "min_reward": -2187.693115234375,
      "std_reward": 311.9819030761719,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 43,
      "timesteps": 842676,
      "mean_reward": -261.2958984375,
      "max_reward": 999.9429931640625,
      "min_reward": -1967.26220703125,
      "std_reward": 442.2179260253906,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19035
    },
    {
      "iteration": 44,
      "timesteps": 862177,
      "mean_reward": -260.7384338378906,
      "max_reward": 999.9235229492188,
      "min_reward": -1368.8505859375,
      "std_reward": 283.4339294433594,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 45,
      "timesteps": 881701,
      "mean_reward": -228.2183380126953,
      "max_reward": 988.0448608398438,
      "min_reward": -556.9655151367188,
      "std_reward": 211.30767822265625,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19524
    },
    {
      "iteration": 46,
      "timesteps": 901701,
      "mean_reward": -287.84698486328125,
      "max_reward": -109.97146606445312,
      "min_reward": -1076.58544921875,
      "std_reward": 193.1625213623047,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 47,
      "timesteps": 920703,
      "mean_reward": -365.3597717285156,
      "max_reward": 999.9176025390625,
      "min_reward": -2821.053466796875,
      "std_reward": 681.667236328125,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19002
    },
    {
      "iteration": 48,
      "timesteps": 940204,
      "mean_reward": -314.8319396972656,
      "max_reward": 999.9359741210938,
      "min_reward": -2961.483154296875,
      "std_reward": 501.5845642089844,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19501
    },
    {
      "iteration": 49,
      "timesteps": 959206,
      "mean_reward": -230.95657348632812,
      "max_reward": 999.923583984375,
      "min_reward": -1973.67431640625,
      "std_reward": 394.37908935546875,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19002
    },
    {
      "iteration": 50,
      "timesteps": 979206,
      "mean_reward": -337.98876953125,
      "max_reward": -119.75371551513672,
      "min_reward": -2120.318359375,
      "std_reward": 351.5357971191406,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 51,
      "timesteps": 999206,
      "mean_reward": -324.1135559082031,
      "max_reward": -142.4587860107422,
      "min_reward": -1248.348388671875,
      "std_reward": 233.23724365234375,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 20000
    },
    {
      "iteration": 52,
      "timesteps": 1018230,
      "mean_reward": -222.9153289794922,
      "max_reward": 999.9172973632812,
      "min_reward": -1680.2462158203125,
      "std_reward": 366.1559143066406,
      "best_reward": -201.1376495361328,
      "episodes": 40,
      "steps_this_iter": 19024
    }
  ],
  "eval_history": [
    {
      "iteration": 50,
      "timesteps": 979206,
      "eval_reward": -236.78238672773696
    }
  ]
}